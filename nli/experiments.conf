basic {
  data_dir = /home/lxu85/xnli_data_dir  # Edit this
  download_dir = ${basic.data_dir}/download  # dir that contains downloaded dataset
  log_root = ${basic.data_dir}
}

#*************** Dataset-specific config ***************
# Do not have overlapping attributes with model config later

dataset = ${basic} {
  max_segment_len = 128
}

#*************** Model-specific config ***************

model {
  # Learning
  num_epochs = 4
  batch_size = 32
  eval_batch_size = 64
  gradient_accumulation_steps = 1
  bert_learning_rate = 2e-5
  adam_eps = 1e-8
  adam_weight_decay = 1e-4
  warmup_ratio = 0.1
  max_grad_norm = 1
  dropout_rate = 0.1
  freeze_emb = false
  dim_reduce = false

  # Uncertainty
  lang_un = false
  un = false
  use_un_probs = true  # Meaningful when un is true
  un_as_regression = false  # true: use un as Kendall'17; false: use un as regression similar to Kendall'18
  evi_un = false  # Evidential un; cannot use together with other uns

  # Other
  eval_frequency = 5000
  report_frequency = 1000

  dataset_name = xnli
  self_learning = false
}

model_sl = ${model} {
  self_learning = true
  sl_max_itr = 8
  sl_num_epochs = 6
  sl_en_ratio = 0.2
  sl_lang_ratio = 0  # Sampling ratio of previous selected; 0 to disable
  sl_criterion = max_prob
  sl_top_k_ratio = 0.01  # For selection; Per lang per class/type
  sl_selection_threshold = false  # For selection
  eval_frequency = 500
  report_frequency = 100
}

mbert_base = ${model}{
  model_type = bert
  pretrained = bert-base-multilingual-cased
}

xlmr_base = ${model}{
  model_type = xlmr
  pretrained = xlm-roberta-base
}

xlmr_large = ${model}{
  model_type = xlmr
  pretrained = xlm-roberta-large
}

xlmr_large_sl = ${model_sl} {
  model_type = xlmr
  pretrained = xlm-roberta-large
  init_config_name = xlmr_large_zero_shot_v2
  init_suffix = May06_21-05-42
  dim_reduce = 128
}

#*************** Experiment-specific config: baseline ***************

mbert_base_zero_shot = ${dataset} ${mbert_base} {
  zero_shot = true
}

xlmr_base_zero_shot = ${dataset} ${xlmr_base} {
  zero_shot = true
}

xlmr_large_zero_shot = ${dataset} ${xlmr_large} {
  zero_shot = true
}

xlmr_large_zero_shot_un = ${xlmr_large_zero_shot} {
  un = true
}

xlmr_large_zero_shot_unr = ${xlmr_large_zero_shot} {
  un = true
  un_as_regression = true
}

xlmr_large_zero_shot_eu = ${xlmr_large_zero_shot} {
  evi_un = true
}

xlmr_large_zero_shot_v2 = ${xlmr_large_zero_shot} {
  dim_reduce = 128
}

xlmr_large_zero_shot_v3 = ${xlmr_large_zero_shot_v2} {
  evi_un = true
}

mt5_large_zero_shot = ${dataset} ${mt5_large} {
  zero_shot = true
}

#*************** Experiment-specific config: SL ***************

xlmr_large_zero_shot_sl = ${dataset} ${xlmr_large_sl} {
  zero_shot = true
  sl_criterion = max_prob
  sl_max_itr = 5
  sl_en_ratio = 0.01
  sl_top_k_ratio = 0.08
  sl_lang_ratio = 1
  sl_num_epochs = 3
}

xlmr_large_zero_shot_sl_lu = ${xlmr_large_zero_shot_sl} {
  lang_un = true
}

xlmr_large_zero_shot_sl_v3 = ${xlmr_large_zero_shot_sl} {
  sl_criterion = entropy
}

xlmr_large_zero_shot_sl_v10 = ${xlmr_large_zero_shot_sl} {
  sl_criterion = entropy
  un = true
}

xlmr_large_zero_shot_sl_v11 = ${xlmr_large_zero_shot_sl} {
  sl_criterion = entropy
  lang_un = true
}

xlmr_large_zero_shot_sl_v12 = ${xlmr_large_zero_shot_sl} {
  sl_criterion = entropy
  un = true
  dim_reduce = false
  init_config_name = xlmr_large_zero_shot_un
  init_suffix = May14_00-11-31
}

xlmr_large_zero_shot_sl_v4 = ${xlmr_large_zero_shot_sl} {
  init_config_name = xlmr_large_zero_shot_v3
  init_suffix = May06_21-05-54
  evi_un = true
  sl_criterion = dissonance
}

xlmr_large_zero_shot_sl_v8 = ${xlmr_large_zero_shot_sl_v4} {
  sl_criterion = entropy
}

xlmr_large_zero_shot_sl_v9 = ${xlmr_large_zero_shot_sl_v4} {
  sl_criterion = custom
}
